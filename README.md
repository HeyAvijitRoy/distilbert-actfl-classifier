# DistilBERT ACTFL-style English Writing Proficiency Classifier

[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python&logoColor=white)](https://www.python.org/) [![PyTorch](https://img.shields.io/badge/PyTorch-%3E%3D2.0.0-black?logo=pytorch&logoColor=white)](https://pytorch.org/) [![Transformers](https://img.shields.io/badge/Transformers-HuggingFace-orange?logo=huggingface&logoColor=white)](https://huggingface.co/transformers/) [![Gradio](https://img.shields.io/badge/Gradio-UI-green?logo=gradio&logoColor=white)](https://gradio.app/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)

**A self-hosted pipeline that trains a DistilBERT-based classifier to map English learner essays to ACTFL-like proficiency buckets (10 classes: Novice Low ‚Üí Superior).** Includes data preparation, augmentation from Hugging Face CommonLit readability data, training with class-weighted loss, evaluation with real confusion matrices, and a simple Gradio demo for inference.

---

## üìã Table of Contents

- [Quick Start (PowerShell)](#quick-start-powershell)
- [At-a-Glance](#at-a-glance)
- [Repository Structure](#repository-structure)
- [Data Sources & Placement](#data-sources--placement)
- [ACTFL Proficiency Levels](#actfl-proficiency-levels)
- [Model Card & Performance](#model-card--performance)
- [Architecture & Training](#architecture--training)
- [Installation](#installation)
- [Usage Guide](#usage-guide)
  - [1. Prepare Labeled Data](#1-prepare-labeled-data)
  - [2. (Optional) Augment with CommonLit](#2-optional-augment-with-commonlit)
  - [3. Train](#3-train)
  - [4. Evaluate](#4-evaluate)
  - [5. Interactive Inference](#5-interactive-inference)
- [Technical Implementation](#technical-implementation)
- [‚ö†Ô∏è Limitations & Reliability](#Ô∏è-limitations--reliability)
- [Contributing & Data Collection](#contributing--data-collection)
- [Troubleshooting](#troubleshooting)
- [License & Contact](#license--contact)

---

## üöÄ Quick Start (PowerShell)

**1. Create & activate virtual environment:**
```powershell
python -m venv venv
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser   # one-time if needed
.\venv\Scripts\Activate.ps1
```

**2. Install dependencies:**
```powershell
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

**3. Prepare labeled data:**
```powershell
python script.py
# ‚Üí creates: asap_actfl_labeled.csv
```

**4. (Optional) Augment with CommonLit high-proficiency samples:**
```powershell
python prepare_hf_augmented.py
# ‚Üí creates: asap_plus_hf.csv
```

**5. Train model:**
```powershell
python train.py
# ‚Üí generates: model_output/ (checkpoints) and distilbert-actfl-english/ (final model)
```

**6. Evaluate:**
```powershell
python eval.py
# ‚Üí prints confusion matrix, metrics, per-class accuracy
```

**7. Launch interactive demo:**
```powershell
python app.py
# ‚Üí opens Gradio UI at http://localhost:7860
```

---

## üìå At-a-Glance

What's in this repository:

‚úÖ **Data pipeline**: ASAP raw data ‚Üí ACTFL-labeled CSV with 10 proficiency bins  
‚úÖ **Augmentation**: Optional CommonLit Ease-of-Readability integration for high-proficiency classes  
‚úÖ **Training**: Hugging Face `Trainer` with class-weighted loss to handle imbalance  
‚úÖ **Evaluation**: Confusion matrix, per-class accuracy, macro F1-score  
‚úÖ **Inference UI**: Gradio app for single-essay predictions  
‚úÖ **Model artifacts**: Saved model (`distilbert-actfl-english/`) and checkpoints (`model_output/`)  
‚úÖ **Reproducibility**: All hyperparameters and random seeds explicit in `train.py`

---

## üìÅ Repository Structure

```
.
‚îú‚îÄ asap-aes/                           # Place ASAP TSV files here (training_set_rel3.tsv, etc.)
‚îú‚îÄ asap_actfl_labeled.csv              # Created by script.py
‚îú‚îÄ asap_plus_hf.csv                    # Created by prepare_hf_augmented.py (optional)
‚îú‚îÄ script.py                           # Label ASAP rubric scores ‚Üí ACTFL bins
‚îú‚îÄ prepare_hf_augmented.py             # Augment with CommonLit readability samples
‚îú‚îÄ train.py                            # Training script (HF Trainer, class-weighted loss)
‚îú‚îÄ eval.py                             # Evaluation script (confusion matrix + metrics)
‚îú‚îÄ app.py                              # Gradio inference UI
‚îú‚îÄ distilbert-actfl-english/           # Final saved model (after training)
‚îú‚îÄ model_output/                       # Trainer checkpoints and logs
‚îú‚îÄ requirements.txt                    # Python dependencies
‚îú‚îÄ LICENSE
‚îú‚îÄ MODEL_CARD.md                       # Detailed model card (datasets, metrics)
‚îî‚îÄ README.md                           # This file
```

---

## üìä Data Sources & Placement

| Dataset | Source | Purpose | Size |
|---------|--------|---------|------|
| **ASAP AES** | [Kaggle Competition](https://www.kaggle.com/competitions/asap-aes) | Primary labeled essays (rubric scores) | ~12,976 examples |
| **CommonLit Ease-of-Readability** | [Hugging Face](https://huggingface.co/datasets/casey-martin/CommonLit-Ease-of-Readability) | Augment high-proficiency classes (Flesch Reading Ease filter) | ~355 Superior + ~354 Advanced High |

**Setup:**
- Download ASAP training/validation TSV files and place in `asap-aes/` folder
- CommonLit is auto-downloaded by `prepare_hf_augmented.py` if augmentation is needed

---

## üó£Ô∏è ACTFL Proficiency Levels

The ACTFL Proficiency Guidelines framework used in this project (10 levels):

| Level | Tier | Description |
|-------|------|-------------|
| **Novice Low** | Beginner | Limited vocabulary, frequent errors, simple sentences |
| **Novice Mid** | Beginner | Emerging patterns, isolated words/phrases, basic structures |
| **Novice High** | Beginner | Simple sentences, common topics, comprehensible with effort |
| **Intermediate Low** | Intermediate | Expanded vocabulary, short paragraphs, generally understandable |
| **Intermediate Mid** | Intermediate | Consistent structures, varied topics, occasional errors |
| **Intermediate High** | Intermediate | Complex ideas, detailed descriptions, minor errors |
| **Advanced Low** | Advanced | Abstract concepts, varied register, sophisticated vocabulary |
| **Advanced Mid** | Advanced | Nuanced expression, cultural references, accurate syntax |
| **Advanced High** | Advanced | Superior organization, idioms, near-native proficiency |
| **Superior** | Mastery | Native-like proficiency, complex discourse, exceptional clarity |

---

## üìã Model Card & Performance

### Best Reported Evaluation (Final Run)

| Metric | Value |
|--------|-------|
| **Accuracy** | 0.9291 (92.91%) |
| **Macro F1** | 0.4417 |
| **Eval Loss** | 0.2392 |

### Per-Class Accuracy (Real Results)

| Label | Correct / Total | Accuracy |
|-------|--------|----------|
| Novice Low | 1819 / 1833 | **99.24%** |
| Novice Mid | 346 / 376 | **92.02%** |
| Novice High | 112 / 142 | **78.87%** |
| Intermediate Low | 65 / 97 | **67.01%** |
| **Intermediate Mid** | **0 / 15** | **0.00%** ‚ö†Ô∏è |
| Intermediate High | 27 / 55 | **49.09%** |
| Advanced Low | 43 / 61 | **70.49%** |
| **Advanced Mid** | **0 / 13** | **0.00%** ‚ö†Ô∏è |
| **Advanced High** | **0 / 3** | **0.00%** ‚ö†Ô∏è |
| **Superior** | **0 / 1** | **0.00%** ‚ö†Ô∏è |

**‚ö†Ô∏è Key Observation:** High overall accuracy (92.91%) is **heavily driven by Novice Low dominance** (1833/2596 = 70% of evaluation set). Higher-proficiency classes are severely under-represented and have poor per-class accuracy. See [Limitations & Reliability](#Ô∏è-limitations--reliability) for critical details.

### Confusion Matrix (Real Data)

```
              Predicted ‚Üí
              NL  NM  NH  IL  IM  IH  AL  AM  AH  Su
Novice Low    1819 14   0   0   0   0   0   0   0   0
Novice Mid      4 346  25   1   0   0   0   0   0   0
Novice High     2  10 112  18   0   0   0   0   0   0
Intermediate L  3   0  27  65   1   1   0   0   0   0
Intermediate M  0   0   1   0   0  11   3   0   0   0
Intermediate H  0   0   0   2   0  27  26   0   0   0
Advanced Low    0   0   0   1   0  17  43   0   0   0
Advanced Mid    0   0   0   0   0   1  12   0   0   0
Advanced High   0   0   0   0   0   0   3   0   0   0
Superior        0   0   0   1   0   0   0   0   0   0
```

---

## üèóÔ∏è Architecture & Training

### Model Specification

- **Base:** `distilbert-base-uncased` (66M parameters, 60% smaller than BERT)
- **Tokenizer:** DistilBERT WordPiece tokenizer
- **Output Head:** Linear layer (768 ‚Üí 10 ACTFL classes)
- **Why DistilBERT?** Efficient, effective (97% BERT performance), production-ready, pre-trained on 12GB+ English text

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Max Sequence Length | 256 tokens (~1000 words) |
| Batch Size | 8 (train & eval) |
| Learning Rate | 2e-5 |
| Epochs | 3 |
| Weight Decay | 0.01 |
| Loss Function | Class-weighted CrossEntropy |
| Optimization | AdamW (Hugging Face Trainer) |
| Train / Validation Split | 80 / 20 |

### Class Weighting

To handle imbalance, the model uses inverse-frequency weighting:

$$\text{weight}_c = \frac{\text{total samples}}{n\_\text{classes} \times \text{samples in class}_c}$$

This forces the model to allocate more learning capacity to underrepresented classes.

---

## üíæ Installation

### Prerequisites

- **Python:** 3.8 or higher
- **CUDA (Optional):** For GPU acceleration (NVIDIA GPU with CUDA 11.8+)
- **Git:** For cloning

### Setup Steps

```powershell
# Step 1: Clone
git clone <repository-url>
cd English

# Step 2: Create venv
python -m venv venv
.\venv\Scripts\Activate.ps1

# Step 3: Install deps
python -m pip install --upgrade pip
python -m pip install -r requirements.txt

# Step 4: Verify
python -c "import torch; print(f'PyTorch: {torch.__version__}'); import transformers; print(f'Transformers: {transformers.__version__}')"
```

**Key Dependencies:**
- `torch` ‚Äî PyTorch framework
- `transformers` ‚Äî Hugging Face models & Trainer
- `datasets` ‚Äî HF datasets library
- `scikit-learn` ‚Äî Evaluation metrics
- `pandas` ‚Äî Data manipulation
- `gradio` ‚Äî Web UI
- `numpy`, `matplotlib` ‚Äî Numerics & plotting

---

## üìñ Usage Guide

### 1. Prepare Labeled Data

```powershell
python script.py
```

**What it does:**
- Reads ASAP TSV files from `asap-aes/`
- Bins `domain1_score` into 10 quantile buckets using `pd.qcut`
- Maps buckets to ACTFL labels: Novice Low, ..., Superior
- Outputs: `asap_actfl_labeled.csv` (essay + actfl_level columns)

**Output example:**
```csv
essay,actfl_level
"The sun is bright and I like it.",Novice Low
"Learning languages is important for communication.",Intermediate Mid
```

---

### 2. (Optional) Augment with CommonLit

```powershell
python prepare_hf_augmented.py
```

**What it does:**
- Downloads CommonLit readability dataset from Hugging Face
- Selects top 10% easiest passages ‚Üí "Superior"
- Selects top 10-20% easiest passages ‚Üí "Advanced High"
- Combines with ASAP data
- Outputs: `asap_plus_hf.csv` (~13,685 total examples)

**Use in training:**
Edit `train.py` line `data_files = {"data": "asap_actfl_labeled.csv"}` to point to `asap_plus_hf.csv`

---

### 3. Train

```powershell
python train.py
```

**What happens:**
1. Loads labeled CSV (80% train, 20% validation)
2. Tokenizes essays (max 256 tokens)
3. Initializes DistilBERT + 10-class head
4. Fine-tunes for 3 epochs with class-weighted loss
5. Saves checkpoints every 500 steps ‚Üí `model_output/checkpoint-*`
6. Saves final model ‚Üí `distilbert-actfl-english/`

**Estimated time:**
- GPU (RTX 3080): 2-3 hours
- CPU: 15-20 hours

---

### 4. Evaluate

```powershell
python eval.py
```

**Output:**
- Confusion matrix (rows = true labels, cols = predictions)
- Per-class accuracy table
- Overall accuracy, macro F1, loss
- Example: See [Model Card & Performance](#model-card--performance) above

---

### 5. Interactive Inference

```powershell
python app.py
```

**Then:**
1. Open browser to printed URL (usually `http://localhost:7860`)
2. Paste or type essay text
3. Get predicted ACTFL level + confidence scores

**Example input:**
```
I went to the store yesterday. I bought milk and bread.
The store was very big. I like shopping there.
```

**Example output:**
```
Predicted ACTFL Level: Novice Mid
Probabilities:
  Novice Low: 0.05
  Novice Mid: 0.82  ‚Üê highest
  Novice High: 0.10
  ...
```

---

## üî¨ Technical Implementation

### Tokenization Pipeline

```python
inputs = tokenizer(
    essay_text,
    truncation=True,          # Truncate to max_length
    padding="max_length",     # Pad to max_length
    max_length=256,           # 256 tokens ‚âà ~1000 words
    return_tensors="pt"       # PyTorch tensors
)
# Returns: input_ids, token_type_ids, attention_mask
```

**Max length rationale:**
- 256 tokens covers ~99% of student essays
- Longer sequences increase memory & training time
- Shorter sequences lose information

### Inference (Forward Pass)

```python
# 1. Tokenize
inputs = tokenizer(text, truncation=True, max_length=256, return_tensors="pt")

# 2. Forward pass (no gradients)
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits  # [batch, 10]

# 3. Probabilities
probs = torch.softmax(logits, dim=-1)  # sum = 1.0

# 4. Top prediction
pred_idx = torch.argmax(probs, dim=-1)
confidence = probs[0, pred_idx]

# 5. Map to ACTFL label
label = ACTFL_LABELS[pred_idx]
```

### Label Mapping (Index ‚Üî Label)

```python
ACTFL_LABELS = [
    "Novice Low",         # 0
    "Novice Mid",         # 1
    "Novice High",        # 2
    "Intermediate Low",   # 3
    "Intermediate Mid",   # 4
    "Intermediate High",  # 5
    "Advanced Low",       # 6
    "Advanced Mid",       # 7
    "Advanced High",      # 8
    "Superior"            # 9
]
```

---

## ‚ö†Ô∏è Limitations & Reliability

### üö® Critical Issue: Severe Class Imbalance

**The model's predictions are heavily skewed by training data distribution.** This is the single most important limitation to understand:

#### The Problem

- **Novice Low:** 1,833 examples (70% of eval set)
- **Novice Mid:** 376 examples (14%)
- **Intermediate High:** 55 examples (2%)
- **Superior:** 1 example (0.04%)

Result: **Novice Low dominates predictions.** Even though overall accuracy is 92.91%, this is almost entirely because the model learned to predict Novice Low accurately. Higher proficiency classes (Superior, Advanced High, Intermediate Mid) have **0% per-class accuracy** due to extreme under-representation.

#### Impact on Usage

‚úÖ **Reliable (Safe to Use):**
- Classifying Novice-level writing (Novice Low/Mid)
- Filtering clearly beginner essays
- Research & development

‚ùå **Unreliable (NOT Safe):**
- Classifying Advanced or Superior writing
- High-stakes decisions (grading, placement)
- Production systems without human review
- Any classification without checking per-class accuracy in confusion matrix

### Class Imbalance: Why Weighting Isn't Enough

This implementation uses **class-weighted loss** during training:

```python
class_weights = torch.tensor([w1, w2, ..., w10])
loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)
```

**What it helps with:**
‚úÖ Prevents collapse of minority classes entirely  
‚úÖ Forces allocation of learning capacity to rare classes  
‚úÖ Improves per-class F1 compared to unweighted baseline

**What it can't fix:**
‚ùå Cannot create information that doesn't exist  
‚ùå Cannot overcome 1800:1 sample ratio (Novice Low : Superior)  
‚ùå Limited by magnitude of imbalance  

**Reality:** With only 1 Superior example vs. 1,833 Novice Low, no amount of weighting can solve the fundamental data shortage.

### Why High Overall Accuracy is Misleading

```
Overall Accuracy = 92.91%
This looks great until you check per-class:

  Novice Low:      99.24% ‚úÖ (1,833 examples)
  Novice Mid:      92.02% ‚úÖ (376 examples)
  Novice High:     78.87% ‚ö†Ô∏è (142 examples)
  Intermediate Low: 67.01% ‚ö†Ô∏è (97 examples)
  Advanced Low:    70.49% ‚ö†Ô∏è (61 examples)
  
  Advanced High:    0.00% ‚ùå (only 3 examples)
  Superior:         0.00% ‚ùå (only 1 example)
  
The 92.91% average is weighted by volume, not represented equally across classes.
Macro F1 (0.44) is more honest ‚Äî it shows average performance across all classes.
```

### Critical Recommendations

**For Research / Development:**
```
‚úÖ Current model is suitable for:
‚îú‚îÄ Exploratory data analysis
‚îú‚îÄ Proof-of-concept prototypes
‚îú‚îÄ Screening coarse Novice-level writing
‚îî‚îÄ Understanding ACTFL classification task
```

**For Production / High-Stakes Use:**
```
‚ùå Current model is NOT suitable without:
‚îú‚îÄ Rebalanced dataset (1,000+ examples per level minimum)
‚îú‚îÄ Expert validation of edge cases
‚îú‚îÄ Per-class accuracy guarantees
‚îú‚îÄ Human review of Advanced/Superior classifications
‚îî‚îÄ Documented fallback (escalate to human if confidence < X%)
```

### Path to Production

Before using this model in production, you MUST:

1. **Collect Balanced Dataset**
   - Goal: ‚â•1,000 essays per ACTFL level (10,000+ total)
   - Use stratified sampling or multi-source collection
   - Current: 2,596 total (highly imbalanced)

2. **Retrain on Balanced Data**
   - Run `train.py` with new dataset
   - Expected improvement: +5-10% per-class accuracy

3. **Validate on Independent Test Set**
   - Use k-fold cross-validation
   - Ensure high per-class accuracy (‚â•80% per level)

4. **Expert Review**
   - Have ACTFL professionals review edge cases
   - Calibrate confidence thresholds

5. **Document & Release v1.0**
   - Update this README with new metrics
   - Tag release, document limitations clearly
   - Publish reproduction steps

### Model Development Status

```
Current Phase: Early Exploration
‚îú‚îÄ Dataset: 2,596 examples (imbalanced)
‚îú‚îÄ Use cases: Research, POC, development
‚îú‚îÄ Production ready: NO
‚îî‚îÄ Recommended: For development use only with caveats

Next Phase: Balanced dataset v1 (planned)
‚îú‚îÄ Target: 5,000 examples (500 per level)
‚îú‚îÄ Augmentation: Text paraphrasing for rare classes
‚îú‚îÄ Retraining: Full pipeline
‚îî‚îÄ Validation: Internal k-fold cross-validation

Final Phase: Production v1.0 (future)
‚îú‚îÄ Target: 15,000+ examples (1,500+ per level)
‚îú‚îÄ Validation: Independent expert review
‚îú‚îÄ Deployment: Docker, API, monitoring
‚îî‚îÄ SLA: Per-class accuracy ‚â•85%
```

---

## ü§ù Contributing & Data Collection

**Contributions are especially welcome** for underrepresented ACTFL levels. Because the current dataset is heavily imbalanced, additional high-quality essays‚Äîespecially at Advanced and Superior levels‚Äîwill directly improve model reliability.

### How to Contribute

**Option 1: Submit Writing Samples**
- Anonymized essays tagged with ACTFL level
- Format: CSV with columns `essay` and `actfl_level`
- Guidelines: 100-200 words minimum, avoid copyrighted text
- Submit via: Pull request, GitHub issue, or secure link

**Option 2: Suggest Augmentation Strategies**
- Paraphrasing techniques for minority classes
- Data collection strategies from new sources
- Rebalancing algorithms
- Submit via: GitHub discussion or issue

**Option 3: Code Contributions**
- Improve evaluation metrics (per-class reports, calibration curves)
- Add new preprocessing (spell-check, grammar filtering)
- Implement ensemble models
- Add API deployment (FastAPI, Docker)
- Submit via: Pull request with description

### Data Guidelines

If submitting essays:
‚úÖ Fully anonymized (no names, IDs, institutions)  
‚úÖ ACTFL level labeled by expert or professional  
‚úÖ 100+ words per sample  
‚úÖ Diverse topics & genres  
‚úÖ Original or properly licensed text

‚ùå No copyrighted text, homework, plagiarized content

---

## üêõ Troubleshooting

### CUDA Out of Memory
```
RuntimeError: CUDA out of memory
```
**Solution:**
```powershell
# In train.py, reduce batch size:
# per_device_train_batch_size=4  # was 8

# Or reduce sequence length:
# max_length=128  # was 256

# Or use gradient accumulation:
# gradient_accumulation_steps=2
```

### Missing Dataset
```
FileNotFoundError: asap_actfl_labeled.csv not found
```
**Solution:**
```powershell
# Run data preparation first:
python script.py
```

### Model Doesn't Load
```
RuntimeError: Cannot find safetensors model
```
**Solution:**
```powershell
# Verify model exists:
ls distilbert-actfl-english/
# Should contain: config.json, model.safetensors, tokenizer.json, vocab.txt

# If missing, retrain:
python train.py
```

### Poor Predictions
**Checklist:**
- Verify model file integrity (try reloading)
- Check input text (remove HTML, fix encoding)
- Consider class imbalance (see [Limitations](#Ô∏è-limitations--reliability))
- For Advanced/Superior: expect low accuracy (see confusion matrix)

### Slow Inference
**Solutions:**
- Use GPU: `torch.cuda.is_available()`
- Quantize model for 4x speedup
- Batch multiple essays together

---

## üìÑ License & Contact

**License:** MIT ‚Äî see `LICENSE` file

**Author:** Avijit Roy  
**Website:** [avijitroy.com](https://avijitroy.com/)  
**LinkedIn:** [/in/HeyAvijitRoy](https://www.linkedin.com/in/HeyAvijitRoy/)

**Questions or issues?** Open a GitHub issue or reach out via LinkedIn.

---

## üìñ Additional Resources

- [ACTFL Proficiency Guidelines](https://www.actfl.org/guidance/actfl-proficiency-guidelines-2012)
- [DistilBERT Paper](https://arxiv.org/abs/1910.01108)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [ASAP Dataset](https://www.kaggle.com/competitions/asap-aes/data)
- [CommonLit Readability](https://www.kaggle.com/datasets/shayanfazeli/commonlit-readability-prize)
- [Gradio Documentation](https://gradio.app/docs)

---

_"Building tools to solve real problems ‚Äî secure, fast, and privacy-first."_
